{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook contains all functions needed for preprocessing FITS files, augmenting images, and splitting datasets.\n",
    "\n",
    "## How to Use the Data Preprocessing Pipeline\n",
    "\n",
    "This notebook contains a series of functions designed to preprocess FITS files, augment images, and split datasets. The pipeline requires a combination of these different functions, and the entire pipeline structure is shown commented out.\n",
    "\n",
    "### General Workflow\n",
    "\n",
    "1. **Convert FITS to PNG (Synthetic Data Only)**: Use the `fits_to_png` function to convert FITS files to PNG format. This step is only necessary for synthetic data.\n",
    "2. **Augment Images**: Use the `data_augmentation` function to apply data augmentation to a directory of images.\n",
    "3. **Split Data**: Use the `split_data` function to split the dataset into training, validation, and test sets.\n",
    "\n",
    "### Synthetic Data\n",
    "\n",
    "Synthetic data requires unique functions to perform additional preprocessing steps, such as converting FITS files to PNG format. Ensure that you run these specific functions for synthetic data preprocessing.\n",
    "\n",
    "### Running Individual Functions\n",
    "\n",
    "In most instances, you will need to run individual functions and move some files around to ensure they are in the correct folder. For example, when you want to train on both synthetic and real images, you may need to:\n",
    "\n",
    "1. **Convert FITS files (Synthetic Data Only)**: Convert FITS files from the synthetic dataset to PNG format.\n",
    "2. **Combine Datasets**: Move the images to a common directory for augmenting.\n",
    "3. **Augment Images**: Apply data augmentation to both synthetic and real images.\n",
    "\n",
    "### Example Pipeline\n",
    "\n",
    "Here is an example of how you might run the pipeline:\n",
    "\n",
    "1. **Convert FITS to PNG (Synthetic Data Only)**:\n",
    "    ```python\n",
    "    convert_fits_from_directory('path/to/synthetic_fits_files', 'path/to/synthetic_png_files', image_size=64)\n",
    "    ```\n",
    "\n",
    "2. **Augment Images**:\n",
    "    ```python\n",
    "    data_augmentation('path/to/png_files', 'path/to/augmented_images', no_of_augmentations=2)\n",
    "    ```\n",
    "\n",
    "3. **Split Data**:\n",
    "    ```python\n",
    "    split_data('path/to/augmented_images', 'path/to/split_data')\n",
    "    ```\n",
    "\n",
    "By following these steps, you can preprocess your data and prepare it for training. Adjust the paths and parameters as needed for your specific use case.\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as conf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "from astropy.visualization import ZScaleInterval\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert FITS to PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fits_to_png(fits_file, output_png, image_size=64):\n",
    "    \"\"\"\n",
    "    Convert a FITS file to PNG and resize it to a specific target size.\n",
    "    \n",
    "    Parameters:\n",
    "    - fits_file (string): path to the input FITS file.\n",
    "    - output_png (string): path to save the PNG file.\n",
    "    - target_size (int): representing the desired image size, e.g., 64.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the FITS file\n",
    "    with fits.open(fits_file) as hdul:\n",
    "        data = hdul[0].data\n",
    "    \n",
    "    # Replace NaNs or Infs with zeros to avoid issues\n",
    "    data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # Apply Zscale normalization to raw data\n",
    "    zscale = ZScaleInterval()\n",
    "    vmin, vmax = zscale.get_limits(data)\n",
    "\n",
    "    # Clip the data to the Zscale range\n",
    "    clipped_data = np.clip(data, vmin, vmax)\n",
    "\n",
    "    # Apply asinh scaling on the clipped data\n",
    "    scaled_data = np.arcsinh(clipped_data)\n",
    "\n",
    "    # Recompute the min/max for the transformed data\n",
    "    scaled_vmin = scaled_data.min()\n",
    "    scaled_vmax = scaled_data.max()\n",
    "\n",
    "    # Normalize the scaled data to the range [0, 255] for 8-bit image representation\n",
    "    scaled_data = (scaled_data - scaled_vmin) / (scaled_vmax - scaled_vmin)\n",
    "    scaled_data = (scaled_data * 255).astype(np.uint8)\n",
    "\n",
    "    # Create an image using PIL\n",
    "    img = Image.fromarray(scaled_data)\n",
    "\n",
    "    # Resize the image if a specific size is provided\n",
    "    if image_size is not None:\n",
    "        img = img.resize((image_size, image_size), Image.LANCZOS)\n",
    "\n",
    "    # Save the image as PNG\n",
    "    img.save(output_png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert FITS Files in Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fits_from_directory(source_folder, output_folder, image_size=64):\n",
    "    \"\"\"\n",
    "    Convert all FITS files in the specified directory to PNG and save them in the output folder.\n",
    "    \n",
    "    Parameters:\n",
    "    - source_folder (string): Directory containing the input FITS files.\n",
    "    - output_folder (string): Folder where the PNG files will be saved.\n",
    "    - image_size (int, int): Tuple representing the desired image size, e.g., (64, 64).\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Iterate over each file in the source directory\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.endswith('.fits'):\n",
    "            fits_file = os.path.join(source_folder, filename)\n",
    "            output_png = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.png\")\n",
    "            fits_to_png(fits_file, output_png, image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Samples by M_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_samples_by_mv(csv_filename):\n",
    "    \"\"\" Gets the negative and positive sample names from a csv file\n",
    "    based upon their M_V values\n",
    "\n",
    "    Args:\n",
    "        csv_filename (str): Name of the csv to extract the names\n",
    "    Returns:\n",
    "        str[], str[]: List of ids for both positive and negative samples\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_filename)\n",
    "\n",
    "    negative_ids = []\n",
    "    positive_ids = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        \n",
    "        if row['M_V'] > conf.NEGATIVE_END:\n",
    "            negative_ids.append(str(row['ID']).zfill(4))\n",
    "        elif row['M_V'] < conf.POSITIVE_START:\n",
    "            positive_ids.append(str(row['ID']).zfill(4))\n",
    "    \n",
    "    return positive_ids, negative_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Filenames\n",
    "\n",
    "Once negative synthetic and positive synthetic have been located, this function converts the name of the object in the csv to the name of the corresponding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filenames(ids):\n",
    "    \"\"\" Adds the correct filename format onto collected ids\n",
    "\n",
    "    Args:\n",
    "        ids (str[]): List of ids\n",
    "\n",
    "    Returns:\n",
    "        str[]: List of filenames\n",
    "    \"\"\"\n",
    "    for i in range(0, len(ids)):\n",
    "        ids[i] = conf.SYNTH_START + ids[i] + conf.SYNTH_END\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place Data in New Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_data_in_new_folder(source_folder, dest_folder, filenames):\n",
    "    \"\"\" Moves files from one folder to another\n",
    "\n",
    "    Args:\n",
    "        source_folder (str): Folder the files are coming from\n",
    "        dest_folder (str): Folder the files are arriving in\n",
    "        filenames (str[]): List of filenames ot move\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(source_folder):\n",
    "        print(f\"Source folder '{source_folder}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(dest_folder):\n",
    "        # Create the destination folder if it doesn't exist\n",
    "        os.makedirs(dest_folder)\n",
    "\n",
    "    # Loop through all files in the source folder\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename in filenames:\n",
    "            # Get the full file path\n",
    "            full_file_name = os.path.join(source_folder, filename)\n",
    "            \n",
    "            # Check if it's a file (not a subdirectory)\n",
    "            if os.path.isfile(full_file_name):\n",
    "                # Copy the file to the destination folder\n",
    "                shutil.copy(full_file_name, dest_folder)\n",
    "                print(f\"Copied: {filename}\")\n",
    "    \n",
    "    print(\"File copying completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(original_images_dir, augmented_images_dir, no_of_augmentations=2):\n",
    "    \"\"\"Applies data augmentation to a directory of images\n",
    "\n",
    "    Args:\n",
    "        original_images_dir (str): Source of the images\n",
    "        augmented_images_dir (str): Destination for the new augmented images\n",
    "        no_of_augmentations (int): How many augmentations should be performed to the data\n",
    "    \"\"\"\n",
    "    # Augmentation properties\n",
    "    datagen = ImageDataGenerator(\n",
    "        width_shift_range=0.2,    # Horizontal shift (10% of the image width)\n",
    "        height_shift_range=0.2,   # Vertical shift (10% of the image height)\n",
    "        rotation_range=15,        # Rotate images by up to 15 degrees\n",
    "        zoom_range=0.1,           # Random zoom in/out by 10%\n",
    "        horizontal_flip=True,     # Randomly flip images horizontally\n",
    "        vertical_flip=False,      # Optionally flip images vertically\n",
    "        fill_mode='wrap'          # Shift without stretching the image\n",
    "    )\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(augmented_images_dir):\n",
    "        os.makedirs(augmented_images_dir)\n",
    "\n",
    "    # Load and augment images\n",
    "    for filename in os.listdir(original_images_dir):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            img_path = os.path.join(original_images_dir, filename)\n",
    "            \n",
    "            # Save original to augmented folder\n",
    "            shutil.copy(img_path, os.path.join(augmented_images_dir, filename))\n",
    "\n",
    "            # Load the image in grayscale\n",
    "            img = load_img(img_path, color_mode='grayscale')  # Load as grayscale\n",
    "            img_array = img_to_array(img)  # Convert to numpy array\n",
    "            img_array = img_array.reshape((1,) + img_array.shape)  # Reshape for the generator\n",
    "            \n",
    "            # Get the base filename (without extension) for saving augmented images\n",
    "            base_filename = os.path.splitext(filename)[0]  # Removes the file extension\n",
    "            # Augment the image but do not save directly\n",
    "            i = 0\n",
    "            for batch in datagen.flow(img_array, batch_size=1):\n",
    "                # Convert back to an image and save in grayscale mode\n",
    "                result_img = Image.fromarray(batch[0].astype(np.uint8).squeeze(), mode='L')\n",
    "                result_img.save(f'{augmented_images_dir}/{base_filename}_aug_{i}.png')\n",
    "                \n",
    "                i += 1\n",
    "                if i >= no_of_augmentations:  # Specifies the amount of augmentation per image\n",
    "                    break\n",
    "\n",
    "    print(\"Data augmentation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(source_dir, dest_dir):\n",
    "    \"\"\"Splits images in the source directory into train, test, and validation sets \n",
    "    and organises them into class subdirectories.\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): Directory containing the original images (organized by class).\n",
    "        dest_dir (str): Directory to save the split datasets (train, test, validate).\n",
    "        train_ratio (float): Proportion of the dataset to be used for training.\n",
    "        test_ratio (float): Proportion of the dataset to be used for testing.\n",
    "        val_ratio (float): Proportion of the dataset to be used for validation.\n",
    "\n",
    "    NOTE: The sum of `train_ratio`, `test_ratio`, and `val_ratio` should be 1.\n",
    "    \"\"\"\n",
    "    # Ensure the ratios sum to 1\n",
    "    if conf.TRAIN_RATIO + conf.TEST_RATIO + conf.VAL_RATIO != 1.0:\n",
    "        raise ValueError(\"Train, test, and validation ratios must sum to 1.\")\n",
    "\n",
    "    # Create destination directories if they don't exist\n",
    "    train_dir = os.path.join(dest_dir, 'train')\n",
    "    test_dir = os.path.join(dest_dir, 'test')\n",
    "    val_dir = os.path.join(dest_dir, 'validate')\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over each class folder in the source directory\n",
    "    for class_name in os.listdir(source_dir):\n",
    "        class_dir = os.path.join(source_dir, class_name)\n",
    "        \n",
    "        # Skip if it's not a directory\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "\n",
    "        # Get all image files in the class directory\n",
    "        all_files = [f for f in os.listdir(class_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "        \n",
    "        # Shuffle the files randomly\n",
    "        random.seed(42)\n",
    "        random.shuffle(all_files)\n",
    "\n",
    "        # Split the files into train, test, and validation sets\n",
    "        train_files, temp_files = train_test_split(all_files, test_size=(1 - conf.TRAIN_RATIO), random_state=42)\n",
    "        test_files, val_files = train_test_split(temp_files, test_size=(conf.VAL_RATIO / (conf.TEST_RATIO + conf.VAL_RATIO)), random_state=42)\n",
    "\n",
    "        # Function to copy files to their respective directories, maintaining class folders\n",
    "        def copy_files(file_list, target_dir):\n",
    "            # Create class subfolder\n",
    "            class_target_dir = os.path.join(target_dir, class_name)\n",
    "            # Create class folder if it doesn't exist\n",
    "            os.makedirs(class_target_dir, exist_ok=True)\n",
    "\n",
    "            for file_name in file_list:\n",
    "                src_path = os.path.join(class_dir, file_name)\n",
    "                dst_path = os.path.join(class_target_dir, file_name)\n",
    "                shutil.copy(src_path, dst_path)\n",
    "\n",
    "        # Copy the files into the respective directories\n",
    "        copy_files(train_files, train_dir)\n",
    "        copy_files(test_files, test_dir)\n",
    "        copy_files(val_files, val_dir)\n",
    "\n",
    "        print(f\"Class '{class_name}' split into train, test, and validate sets.\")\n",
    "        print(f\"  Training set: {len(train_files)} images\")\n",
    "        print(f\"  Test set: {len(test_files)} images\")\n",
    "        print(f\"  Validation set: {len(val_files)} images\")\n",
    "\n",
    "    print(\"Data split complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize Images in Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images_in_folder(source_folder, target_folder, target_size=(64, 64)):\n",
    "    \"\"\"Resizes all images in the source folder to the target size and saves \n",
    "    them in the target folder.\n",
    "\n",
    "    Args:\n",
    "        source_folder (str): Path to the folder containing the original images.\n",
    "        target_folder (str): Path to the folder to save the resized images.\n",
    "        target_size (tuple): Desired image size, e.g., (64, 64).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            img_path = os.path.join(source_folder, filename)\n",
    "            img = Image.open(img_path)\n",
    "            img_resized = img.resize(target_size, Image.LANCZOS)\n",
    "            img_resized.save(os.path.join(target_folder, filename))\n",
    "            print(f\"Resized and saved {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution\n",
    "\n",
    "Pipeline can be ran here, entire pipeline structure is shown commented out. The pipeline will require a combination of these different functions.\n",
    "\n",
    "Synthetic data requires unique functions to do some additional preprocessing steps.\n",
    "\n",
    "In most instances you will need to run individual functions and move some files around so they are in the correct folder. For example when you want to train on both synthetic and real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing pipeline\n",
      "Class 'cluster' split into train, test, and validate sets.\n",
      "  Training set: 7632 images\n",
      "  Test set: 1636 images\n",
      "  Validation set: 1636 images\n",
      "Class 'non-cluster' split into train, test, and validate sets.\n",
      "  Training set: 7027 images\n",
      "  Test set: 1506 images\n",
      "  Validation set: 1507 images\n",
      "Data split complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Data processing pipeline\")\n",
    "    \n",
    "    # FOR SYNTHETIC PREPROCESSING\n",
    "    \n",
    "    # Get positive and negative samples\n",
    "    # pos, neg = collect_samples_by_mv('./data/csv/synthetic_clusters_ordby_M_V.csv')\n",
    "    # # # # Create filenames\n",
    "    # pos = generate_filenames(pos)\n",
    "    # neg = generate_filenames(neg)\n",
    "    # print('Located Cluster and Non-Cluster Files')\n",
    "    # # Create new folders\n",
    "    # place_data_in_new_folder(conf.SYNTH_SOURCE, conf.SYNTH_DEST_POS, pos)\n",
    "    # place_data_in_new_folder(conf.SYNTH_SOURCE, conf.SYNTH_DEST_NEG, neg)\n",
    "    # print('Moved Files')\n",
    "    # # # Convert the files to images\n",
    "    # convert_fits_from_directory(conf.SYNTH_DEST_POS , conf.SYNTH_DEST_POS + '_png', conf.IMAGE_SIZE)\n",
    "    # convert_fits_from_directory(conf.SYNTH_DEST_NEG , conf.SYNTH_DEST_NEG + '_png', conf.IMAGE_SIZE)\n",
    "    # print('Converted Fits files to png')\n",
    "    \n",
    "    # END OF SYNTH PREPROCESSING\n",
    "    \n",
    "    # Resize the images\n",
    "    # resize_images_in_folder(\"../data/images/128_synth_real/cluster\", \"../data/images/64_synth_real/cluster\")\n",
    "    # resize_images_in_folder(\"../data/images/128_synth/non-cluster\", \"../data/images/64_synth/non-cluster\")\n",
    "    # Augment the data\n",
    "    #data_augmentation('../data/images/128_synth_real/cluster', '../data/images/128_synth_real/cluster_aug', 8)\n",
    "    #data_augmentation('../data/images/128_synth_real/non-cluster', '../data/images/128_synth_real/non-cluster_aug', 12)\n",
    "    # print('Augmented Data')\n",
    "    # Split the data\n",
    "    split_data('../data/images/128_synth', '../data/images/128_synth_split')\n",
    "    # print('Split Data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
